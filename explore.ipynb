{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üìöImporting libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "import string\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to /home/gitpod/nltk_data...\n",
                        "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
                        "[nltk_data] Downloading package wordnet to /home/gitpod/nltk_data...\n",
                        "[nltk_data] Downloading package stopwords to /home/gitpod/nltk_data...\n",
                        "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import nltk\n",
                "nltk.download('punkt')\n",
                "import nltk\n",
                "nltk.download('wordnet')\n",
                "import nltk\n",
                "nltk.download('stopwords')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "‚ú®Data Ingestion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>package_name</th>\n",
                            "      <th>review</th>\n",
                            "      <th>polarity</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>com.facebook.katana</td>\n",
                            "      <td>privacy at least put some option appear offli...</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>com.facebook.katana</td>\n",
                            "      <td>messenger issues ever since the last update, ...</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>com.facebook.katana</td>\n",
                            "      <td>profile any time my wife or anybody has more ...</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>com.facebook.katana</td>\n",
                            "      <td>the new features suck for those of us who don...</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>com.facebook.katana</td>\n",
                            "      <td>forced reload on uploading pic on replying co...</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "          package_name                                             review  \\\n",
                            "0  com.facebook.katana   privacy at least put some option appear offli...   \n",
                            "1  com.facebook.katana   messenger issues ever since the last update, ...   \n",
                            "2  com.facebook.katana   profile any time my wife or anybody has more ...   \n",
                            "3  com.facebook.katana   the new features suck for those of us who don...   \n",
                            "4  com.facebook.katana   forced reload on uploading pic on replying co...   \n",
                            "\n",
                            "   polarity  \n",
                            "0         0  \n",
                            "1         0  \n",
                            "2         0  \n",
                            "3         0  \n",
                            "4         0  "
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "total_data = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/naive-bayes-project-tutorial/main/playstore_reviews.csv\")\n",
                "total_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "‚ú®Preprocessing: Tokenization, removal of stop words and special characters, text normalization, and vectorization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Index(['package_name', 'review', 'polarity'], dtype='object')"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "total_data.columns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üë©‚ÄçüíªStep 1: Tokenization and Stop Words Removal\n",
                "Tokenization: Divide the text into smaller units, such as words or phrases (tokens)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                              review  polarity  \\\n",
                        "0   privacy at least put some option appear offli...         0   \n",
                        "1   messenger issues ever since the last update, ...         0   \n",
                        "2   profile any time my wife or anybody has more ...         0   \n",
                        "3   the new features suck for those of us who don...         0   \n",
                        "4   forced reload on uploading pic on replying co...         0   \n",
                        "\n",
                        "                                      processed_text  \n",
                        "0  privacy least put option appear offline mean p...  \n",
                        "1  messenger issues ever since last update initia...  \n",
                        "2  profile time wife anybody one post view would ...  \n",
                        "3  new features suck us working back button guys ...  \n",
                        "4  forced reload uploading pic replying comment l...  \n"
                    ]
                }
            ],
            "source": [
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "def preprocess_text(text):\n",
                "    # Tokenization\n",
                "    tokens = word_tokenize(text)\n",
                "    # Stop Words Removal\n",
                "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
                "    return ' '.join(tokens)\n",
                "\n",
                "total_data['processed_text'] = total_data['review'].apply(preprocess_text)\n",
                "#Show first 10 processed rows\n",
                "print(total_data[['review', 'polarity', 'processed_text']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üë©‚ÄçüíªStep 2: Lemmatization (Optional)\n",
                "Lemmatization: Reduces words to their base forms (lemmas). For example, it converts \"running\" to \"run.\" Helps reduce the dimensions of the feature space by grouping related words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "lemmatizer = WordNetLemmatizer()\n",
                "total_data['processed_text'] = total_data['processed_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üë©‚ÄçüíªStep 3: Removal of Special Characters and Punctuation\n",
                "Removal of Special Characters and Punctuation: Removes punctuation marks and special characters that generally do not contribute information to the task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_data['processed_text'] = total_data['processed_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üë©‚ÄçüíªStep 4: Text Normalization (Convert to lowercase)\n",
                "\n",
                "Text Normalization: Converts all text to lowercase. Helps treat uppercase and lowercase words as identical, reducing complexity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_data['processed_text'] = total_data['processed_text'].apply(lambda x: x.lower())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üë©‚ÄçüíªStep 5: Text Vectorization (Using TF-IDF)\n",
                "\n",
                "Text Vectorization (TF-IDF): Converts text into a numerical representation. TF-IDF assigns scores to words based on their frequency in a document and their inverse frequency in the corpus. This captures the relative importance of words in the document and the corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer(max_features=5000)  # Ajusta el n√∫mero m√°ximo de caracter√≠sticas seg√∫n sea necesario\n",
                "X = vectorizer.fit_transform(total_data['processed_text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üë©‚ÄçüíªStep 6: Removal of Rare or Very Frequent Words (Optional)\n",
                "\n",
                "Removal of Rare or Very Frequent Words: May be optional, but helps eliminate words that are very rare or very common in the corpus. Extremely rare words may not provide useful information, while extremely common words may not be discriminative"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=5000)\n",
                "X = vectorizer.fit_transform(total_data['processed_text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Divide intro train and test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = total_data[\"review\"]\n",
                "y = total_data[\"polarity\"]\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
                "X_train.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Save train and test data on disk."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train.to_csv('X_train.csv', index=False, header=True)\n",
                "X_test.to_csv('X_test.csv', index=False, header=True)\n",
                "y_train.to_csv('y_train.csv', index=False, header=True)\n",
                "y_test.to_csv('y_test.csv', index=False, header=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Transform the test into a matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "vec_model = CountVectorizer(stop_words = \"english\")\n",
                "X_train = vec_model.fit_transform(X_train).toarray()\n",
                "X_test = vec_model.transform(X_test).toarray()\n",
                "\n",
                "X_train"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Naive Bayes üëâ MultinomialNB"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import MultinomialNB\n",
                "\n",
                "model = MultinomialNB()\n",
                "model.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict(X_test)\n",
                "y_pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "accuracy_score(y_test, y_pred)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Save model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the model to disk\n",
                "import pickle\n",
                "with open('multinomial_nb_model.pkl', 'wb') as model_file:\n",
                "    pickle.dump(model, model_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Accuracy counter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
                "\n",
                "for model_aux in [GaussianNB(), BernoulliNB()]:\n",
                "    model_aux.fit(X_train, y_train)\n",
                "    y_pred_aux = model_aux.predict(X_test)\n",
                "    print(f\"{model_aux} with accuracy: {accuracy_score(y_test, y_pred_aux)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "So, the best model should be: MultinomialNB reaching üëâ 0.81564245810055 of accuracy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hyperparams Tunning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "\n",
                "hyperparams = {\n",
                "    \"alpha\": np.linspace(0.01, 10.0, 200),\n",
                "    \"fit_prior\": [True, False],\n",
                "    #\"class_prior\": [None, [0.3, 0.7]],  # Vlues\n",
                "}\n",
                "\n",
                "random_search = RandomizedSearchCV(model, hyperparams, n_iter = 50, scoring = \"accuracy\", cv = 5, random_state = 42)\n",
                "random_search"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Printing best Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_search.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best hyperparameters: {random_search.best_params_}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Accuracy testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = MultinomialNB(alpha = 1.917638190954774, fit_prior = False)\n",
                "model.fit(X_train, y_train)\n",
                "model.fit(X_train, y_train)\n",
                "y_pred = model.predict(X_test)\n",
                "accuracy_score(y_test, y_pred)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Now the model is performing something better than the one above: \n",
                "Without hyperparams tunning üëâ0.81564245810055\n",
                "With hyperparams tunning üëâ 0.8212290502793296"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Save optimized model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f'Accuracy del modelo: {accuracy}')\n",
                "\n",
                "with open('multinomial_nb_model_with_tunning_fit_prior_False_alpha_1.917638190954774.pkl', 'wb') as model_info_file:\n",
                "    pickle.dump({'model': model, 'accuracy': accuracy}, model_info_file)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
