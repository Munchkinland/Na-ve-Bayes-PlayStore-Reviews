# -*- coding: utf-8 -*-
"""NayveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d2BPok66TL99m2StCwHIIqXgUUedu5uc
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

import nltk
nltk.download('punkt')
import nltk
nltk.download('wordnet')
import nltk
nltk.download('stopwords')

import pandas as pd

total_data = pd.read_csv("https://raw.githubusercontent.com/4GeeksAcademy/naive-bayes-project-tutorial/main/playstore_reviews.csv")

total_data.head()

"""Preprocesamiento"""

total_data.columns

"""Tokenización, eliminación de stop words y caracteres especiales, normalizacion de texto y vectorizacion"""

# Paso 1: Tokenización y Eliminación de Stop Words
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenización
    tokens = word_tokenize(text)
    # Eliminación de Stop Words
    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]
    return ' '.join(tokens)

total_data['processed_text'] = total_data['review'].apply(preprocess_text)

# Mostrar las primeras filas del DataFrame con el texto procesado
print(total_data[['review', 'polarity', 'processed_text']].head())

# Paso 2: Lematización (Opcional)
# Lemmatización con NLTK
lemmatizer = WordNetLemmatizer()
total_data['processed_text'] = total_data['processed_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

# Paso 3: Eliminación de Caracteres Especiales y Puntuación
total_data['processed_text'] = total_data['processed_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))

# Paso 4: Normalización de Texto (Convierte a minúsculas)
total_data['processed_text'] = total_data['processed_text'].apply(lambda x: x.lower())

# Paso 5: Vectorización de Texto (Utilizando TF-IDF)
vectorizer = TfidfVectorizer(max_features=5000)  # Ajusta el número máximo de características según sea necesario
X = vectorizer.fit_transform(total_data['processed_text'])

# Paso 7: Eliminación de Palabras Raras o Muy Frecuentes (Opcional)
vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=5000)
X = vectorizer.fit_transform(total_data['processed_text'])

"""👩‍💻Paso 1: Tokenización y Eliminación de Stop Words

Tokenización: Divide el texto en unidades más pequeñas, como palabras o frases (tokens).

Eliminación de Stop Words: Elimina palabras comunes que generalmente no aportan información significativa para la tarea, como "the", "and", "is", etc.

👩‍💻Paso 2: Lematización (Opcional)

Lematización: Reduce las palabras a sus formas base (lemas). Por ejemplo, convierte "running" a "run". Ayuda a reducir las dimensiones del espacio de características al agrupar palabras relacionadas.

👩‍💻Paso 3: Eliminación de Caracteres Especiales y Puntuación
Eliminación de Caracteres Especiales y Puntuación: Elimina signos de puntuación y caracteres especiales que generalmente no aportan información a la tarea.

👩‍💻Paso 4: Normalización de Texto (Convierte a minúsculas)

Normalización de Texto: Convierte todo el texto a minúsculas. Ayuda a tratar las palabras en mayúsculas y minúsculas como idénticas, reduciendo la complejidad.

👩‍💻Paso 5: Vectorización de Texto (Utilizando TF-IDF)

Vectorización de Texto (TF-IDF): Convierte el texto en una representación numérica. TF-IDF asigna puntuaciones a palabras basadas en su frecuencia en un documento y su frecuencia inversa en el corpus. Esto captura la importancia relativa de las palabras en el documento y en el corpus.

👩‍💻Paso 6: Eliminación de Palabras Raras o Muy Frecuentes (Opcional)

Eliminación de Palabras Raras o Muy Frecuentes: Puede ser opcional, pero ayuda a eliminar palabras que son muy raras o muy comunes en el corpus. Palabras extremadamente raras pueden no aportar información útil, mientras que las palabras extremadamente comunes pueden no ser discriminativas

Divide into train & test
"""

X = total_data["review"]
y = total_data["polarity"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train.head()

# Guardar los conjuntos de entrenamiento y prueba a archivos CSV
X_train.to_csv('X_train.csv', index=False, header=True)
X_test.to_csv('X_test.csv', index=False, header=True)
y_train.to_csv('y_train.csv', index=False, header=True)
y_test.to_csv('y_test.csv', index=False, header=True)

"""Transform the test into a matrix"""

from sklearn.feature_extraction.text import CountVectorizer
vec_model = CountVectorizer(stop_words = "english")
X_train = vec_model.fit_transform(X_train).toarray()
X_test = vec_model.transform(X_test).toarray()

X_train

"""Naive Bayes 👉 MultinomialNB"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

# Guardar el modelo a un archivo
import pickle
with open('multinomial_nb_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

"""Accuracy counter"""

from sklearn.naive_bayes import GaussianNB, BernoulliNB

for model_aux in [GaussianNB(), BernoulliNB()]:
    model_aux.fit(X_train, y_train)
    y_pred_aux = model_aux.predict(X_test)
    print(f"{model_aux} with accuracy: {accuracy_score(y_test, y_pred_aux)}")

#So, the best model should be: MultinomialNB

"""Hyperparams Tunning"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV

hyperparams = {
    "alpha": np.linspace(0.01, 10.0, 200),
    "fit_prior": [True, False],
    #"class_prior": [None, [0.3, 0.7]],  # Vlues


}

random_search = RandomizedSearchCV(model, hyperparams, n_iter = 50, scoring = "accuracy", cv = 5, random_state = 42)
random_search

"""Printing best hyperparams

Eliminamos class prior porque penaliza la performance
"""

random_search.fit(X_train, y_train)

print(f"Best hyperparameters: {random_search.best_params_}")

model = MultinomialNB(alpha = 1.917638190954774, fit_prior = False)
model.fit(X_train, y_train)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy_score(y_test, y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy del modelo: {accuracy}')

# Guardar el modelo y su rendimiento en un archivo con pickle
with open('multinomial_nb_model_with_info.pkl', 'wb') as model_info_file:
    pickle.dump({'model': model, 'accuracy': accuracy}, model_info_file)