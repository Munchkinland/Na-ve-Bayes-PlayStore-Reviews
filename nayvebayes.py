# -*- coding: utf-8 -*-
"""NayveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d2BPok66TL99m2StCwHIIqXgUUedu5uc
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

import nltk
nltk.download('punkt')
import nltk
nltk.download('wordnet')
import nltk
nltk.download('stopwords')

import pandas as pd

total_data = pd.read_csv("https://raw.githubusercontent.com/4GeeksAcademy/naive-bayes-project-tutorial/main/playstore_reviews.csv")

total_data.head()

"""Preprocesamiento"""

total_data.columns

"""Tokenizaci贸n, eliminaci贸n de stop words y caracteres especiales, normalizacion de texto y vectorizacion"""

# Paso 1: Tokenizaci贸n y Eliminaci贸n de Stop Words
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenizaci贸n
    tokens = word_tokenize(text)
    # Eliminaci贸n de Stop Words
    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]
    return ' '.join(tokens)

total_data['processed_text'] = total_data['review'].apply(preprocess_text)

# Mostrar las primeras filas del DataFrame con el texto procesado
print(total_data[['review', 'polarity', 'processed_text']].head())

# Paso 2: Lematizaci贸n (Opcional)
# Lemmatizaci贸n con NLTK
lemmatizer = WordNetLemmatizer()
total_data['processed_text'] = total_data['processed_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

# Paso 3: Eliminaci贸n de Caracteres Especiales y Puntuaci贸n
total_data['processed_text'] = total_data['processed_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))

# Paso 4: Normalizaci贸n de Texto (Convierte a min煤sculas)
total_data['processed_text'] = total_data['processed_text'].apply(lambda x: x.lower())

# Paso 5: Vectorizaci贸n de Texto (Utilizando TF-IDF)
vectorizer = TfidfVectorizer(max_features=5000)  # Ajusta el n煤mero m谩ximo de caracter铆sticas seg煤n sea necesario
X = vectorizer.fit_transform(total_data['processed_text'])

# Paso 7: Eliminaci贸n de Palabras Raras o Muy Frecuentes (Opcional)
vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=5000)
X = vectorizer.fit_transform(total_data['processed_text'])

"""┾Paso 1: Tokenizaci贸n y Eliminaci贸n de Stop Words

Tokenizaci贸n: Divide el texto en unidades m谩s peque帽as, como palabras o frases (tokens).

Eliminaci贸n de Stop Words: Elimina palabras comunes que generalmente no aportan informaci贸n significativa para la tarea, como "the", "and", "is", etc.

┾Paso 2: Lematizaci贸n (Opcional)

Lematizaci贸n: Reduce las palabras a sus formas base (lemas). Por ejemplo, convierte "running" a "run". Ayuda a reducir las dimensiones del espacio de caracter铆sticas al agrupar palabras relacionadas.

┾Paso 3: Eliminaci贸n de Caracteres Especiales y Puntuaci贸n
Eliminaci贸n de Caracteres Especiales y Puntuaci贸n: Elimina signos de puntuaci贸n y caracteres especiales que generalmente no aportan informaci贸n a la tarea.

┾Paso 4: Normalizaci贸n de Texto (Convierte a min煤sculas)

Normalizaci贸n de Texto: Convierte todo el texto a min煤sculas. Ayuda a tratar las palabras en may煤sculas y min煤sculas como id茅nticas, reduciendo la complejidad.

┾Paso 5: Vectorizaci贸n de Texto (Utilizando TF-IDF)

Vectorizaci贸n de Texto (TF-IDF): Convierte el texto en una representaci贸n num茅rica. TF-IDF asigna puntuaciones a palabras basadas en su frecuencia en un documento y su frecuencia inversa en el corpus. Esto captura la importancia relativa de las palabras en el documento y en el corpus.

┾Paso 6: Eliminaci贸n de Palabras Raras o Muy Frecuentes (Opcional)

Eliminaci贸n de Palabras Raras o Muy Frecuentes: Puede ser opcional, pero ayuda a eliminar palabras que son muy raras o muy comunes en el corpus. Palabras extremadamente raras pueden no aportar informaci贸n 煤til, mientras que las palabras extremadamente comunes pueden no ser discriminativas

Divide into train & test
"""

X = total_data["review"]
y = total_data["polarity"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train.head()

# Guardar los conjuntos de entrenamiento y prueba a archivos CSV
X_train.to_csv('X_train.csv', index=False, header=True)
X_test.to_csv('X_test.csv', index=False, header=True)
y_train.to_csv('y_train.csv', index=False, header=True)
y_test.to_csv('y_test.csv', index=False, header=True)

"""Transform the test into a matrix"""

from sklearn.feature_extraction.text import CountVectorizer
vec_model = CountVectorizer(stop_words = "english")
X_train = vec_model.fit_transform(X_train).toarray()
X_test = vec_model.transform(X_test).toarray()

X_train

"""Naive Bayes  MultinomialNB"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

# Guardar el modelo a un archivo
import pickle
with open('multinomial_nb_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

"""Accuracy counter"""

from sklearn.naive_bayes import GaussianNB, BernoulliNB

for model_aux in [GaussianNB(), BernoulliNB()]:
    model_aux.fit(X_train, y_train)
    y_pred_aux = model_aux.predict(X_test)
    print(f"{model_aux} with accuracy: {accuracy_score(y_test, y_pred_aux)}")

#So, the best model should be: MultinomialNB

"""Hyperparams Tunning"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV

hyperparams = {
    "alpha": np.linspace(0.01, 10.0, 200),
    "fit_prior": [True, False],
    #"class_prior": [None, [0.3, 0.7]],  # Vlues


}

random_search = RandomizedSearchCV(model, hyperparams, n_iter = 50, scoring = "accuracy", cv = 5, random_state = 42)
random_search

"""Printing best hyperparams

Eliminamos class prior porque penaliza la performance
"""

random_search.fit(X_train, y_train)

print(f"Best hyperparameters: {random_search.best_params_}")

model = MultinomialNB(alpha = 1.917638190954774, fit_prior = False)
model.fit(X_train, y_train)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy_score(y_test, y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy del modelo: {accuracy}')

# Guardar el modelo y su rendimiento en un archivo con pickle
with open('multinomial_nb_model_with_info.pkl', 'wb') as model_info_file:
    pickle.dump({'model': model, 'accuracy': accuracy}, model_info_file)